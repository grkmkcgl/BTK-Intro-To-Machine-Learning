{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMIeymsmKO2846B9D7MJ5D"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Natural Language Processing**"
      ],
      "metadata": {
        "id": "aXRpPC4CR_i9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can't just simply give text to machine. First we need to *preprocess data*. Delete unnecessary information like basketball and Basketball, for computer these are two different word, stop words, parsers...\n",
        "\n",
        "Secondly we will do *feature extraction*. Examples are: word count, word group, N-gram, TF-IDF...\n",
        "\n",
        "Then we can give our information to machine algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "We will use ***NLTK*** for above operations."
      ],
      "metadata": {
        "id": "GfxXcALUTDu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "USE \"Restaurant_Reviews.csv\""
      ],
      "metadata": {
        "id": "9DvhN0KioiXI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "21VyYa7VR87Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48793ef-e9e0-4fd4-9ec5-3fc699f18afc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                 Review  Liked\n",
              "0                             Wow... Loved this place.    1.0\n",
              "1                                   Crust is not good.    0.0\n",
              "2            Not tasty and the texture was just nasty.    0.0\n",
              "3    Stopped by during the late May bank holiday of...    1.0\n",
              "4    The selection on the menu was great and so wer...    1.0\n",
              "..                                                 ...    ...\n",
              "711            the presentation of the food was awful.    0.0\n",
              "712           I can't tell you how disappointed I was.    0.0\n",
              "713  I think food should have flavor and texture an...    0.0\n",
              "714                           Appetite instantly gone.    0.0\n",
              "715  Overall I was not impressed and would not go b...    0.0\n",
              "\n",
              "[716 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"Restaurant_Reviews.txt\", on_bad_lines='skip')\n",
        "data.head"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREPROCESSING**"
      ],
      "metadata": {
        "id": "_6uUCmZSxALA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import regular expression\n",
        "import re\n",
        "\n",
        "# import libraries delete stop words (that, that, did etc.) and suffixes\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "results = []\n",
        "for i in range(len(data)):\n",
        "# delete everything except characters between a-z and A-Z and change with ' '\n",
        "  comment = re.sub('[^a-zA-Z]', ' ', data['Review'][i])\n",
        "\n",
        "# deal with uppercase lowercase characters, make them all same\n",
        "  comment = comment.lower()\n",
        "\n",
        "# convert to list (it'll also get rid of unncessary empty characters)\n",
        "  comment = comment.split()\n",
        "\n",
        "# eliminate suffixes\n",
        "  comment = [ps.stem(word) for word in comment if not word in set(stopwords.words('english'))]\n",
        "# set is used because it won't allow duplicate words and it's unordered\n",
        "\n",
        "# turn list to string\n",
        "  comment = ' '.join(comment)\n",
        "\n",
        "  results.append(comment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usdeOIOHrhS4",
        "outputId": "01eea641-b468-4395-8cf4-79f306562185"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FEATURE EXTRACTION**\n",
        "\n",
        "Count vectorizer is a method to convert text to numerical data [info](https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c#:~:text=Countvectorizer%20is%20a%20method%20to,sparse%20matrix%20as%20shown%20below.)\n"
      ],
      "metadata": {
        "id": "VFRG7a2axD5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words (BOW)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# max_features select 1000 word max\n",
        "cv = CountVectorizer(max_features=2000)\n",
        "\n",
        "# train and transform it'll create a sparse matrix\n",
        "# sparse matrix: a matrix is mostly empty\n",
        "X = cv.fit_transform(results).toarray()\n",
        "\n",
        "# dependent variable\n",
        "y = data.iloc[:,1].values"
      ],
      "metadata": {
        "id": "oji835sQxfQo"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLASSIFICATION**"
      ],
      "metadata": {
        "id": "O2obN7cky2EB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 0)\n",
        "\n",
        "y_train = np.nan_to_num(y_train, nan=0.0)\n",
        "y_test = np.nan_to_num(y_test, nan=0.0)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "dtc.fit(X_train, y_train)\n",
        "y_pred = dtc.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm) # 69,44% acc\n",
        "\n",
        "\n",
        "# COULDN'T FIX\n",
        "# Input contains NaN, infinity or a value too large for dtype('float64').\n",
        "# ISSUE\n",
        "# X_train is not contain any NaN or inf and biggest element is 3.\n",
        "# Other algorithm different than tree is giving same results.\n",
        "\n",
        "# fixed the issue, was looking for X_train but y_train and y_test was including \n",
        "# NaN rows."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gLtE6x5zAW5",
        "outputId": "f4ace1ac-72a1-4c5a-b25d-1be319ff8240"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "False\n",
            "[[56 19]\n",
            " [25 44]]\n"
          ]
        }
      ]
    }
  ]
}