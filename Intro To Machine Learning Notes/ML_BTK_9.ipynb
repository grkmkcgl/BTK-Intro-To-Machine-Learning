{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7Pz00wTNXD7fOZeqGIZGz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Trained ML Model"
      ],
      "metadata": {
        "id": "6SwVVaU5CaNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use **pickle**, joblib, pmml.\n",
        "\n",
        "pmml is also important, it supports multiple platforms(don't sure)"
      ],
      "metadata": {
        "id": "TuYi6w63CgWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"satislar.txt\")\n",
        "\n",
        "X = data.iloc[:,0:1].values\n",
        "Y = data.iloc[:,1].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.33)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "print(lr.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b_IN3vrDoK-",
        "outputId": "db8271fe-951c-4d27-8128-0f4182d955bb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[26677.68883956 35451.59012209 21094.29711432 45820.74618325\n",
            " 26677.68883956 40237.35445801 25880.06145024 40237.35445801\n",
            " 18701.41494636 21891.92450364]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "import pickle\n",
        "model_file = \"model.save\"\n",
        "# wb: write binary\n",
        "pickle.dump(lr,open(model_file,\"wb\"))\n",
        "# also can say: pickle.dump(lr,open(\"model_save\",\"wb\"))\n",
        "\n",
        "\n",
        "# load model back\n",
        "loaded_model = pickle.load(open(model_file,\"rb\"))\n",
        "# test loaded model\n",
        "print(loaded_model.predict(X_test))\n",
        "\n",
        "\n",
        "# NOTICE BOTH PREDICTS ARE SAME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwf6OCYsFi3X",
        "outputId": "ed1557d9-8b8e-4d7f-e705-18c613fbfc0f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[26677.68883956 35451.59012209 21094.29711432 45820.74618325\n",
            " 26677.68883956 40237.35445801 25880.06145024 40237.35445801\n",
            " 18701.41494636 21891.92450364]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:\n",
        "# SAVE DEEP LEARNING MODEL AND REUSE\n",
        "\n",
        "# Same codes with ML BTK-8 deep learning part.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"Churn_Modelling.txt\")\n",
        "data.head()\n",
        "\n",
        "X = data.iloc[:,3:13].values\n",
        "y = data.iloc[:,13].values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le1 = LabelEncoder()\n",
        "X[:,1] = le1.fit_transform(X[:,1])\n",
        "le2 = LabelEncoder()\n",
        "X[:,2] = le2.fit_transform(X[:,2])\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "ohe = ColumnTransformer([(\"ohe\", OneHotEncoder(dtype=float), [1])], \n",
        "                        remainder=\"passthrough\")\n",
        "X = ohe.fit_transform(X)\n",
        "X = X[:,1:]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33,\n",
        "                                                    random_state = 0)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_train = sc.fit_transform(x_train)\n",
        "X_test = sc.fit_transform(x_test)\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(6, kernel_initializer=\"uniform\", activation=\"relu\",\n",
        "                     input_dim=11))\n",
        "classifier.add(Dense(6, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "classifier.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "classifier.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",\n",
        "                   metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "qOBJwSedDFHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN (Convolutional Neural Network)"
      ],
      "metadata": {
        "id": "gBcvB5mXGPv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN based on finding differences.\n",
        "\n",
        "CNN can do *feature extraction* and *machine learning* **itself**.\n",
        "\n",
        "convolution -> pooling(or downsampling) -> flattening -> prediction"
      ],
      "metadata": {
        "id": "cgpw-fUyGRdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Codes are written by Şadi Evren Şeker\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n",
        "# ilkleme\n",
        "classifier = Sequential()\n",
        "\n",
        "# Adım 1 - Convolution\n",
        "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
        "\n",
        "# Adım 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# 2. convolution katmanı\n",
        "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adım 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Adım 4 - YSA\n",
        "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
        "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
        "\n",
        "# CNN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# CNN ve resimler\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('veriler/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 1,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('veriler/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 1,\n",
        "                                            class_mode = 'binary')\n",
        "\n",
        "classifier.fit_generator(training_set,\n",
        "                         samples_per_epoch = 8000,\n",
        "                         nb_epoch = 1,\n",
        "                         validation_data = test_set,\n",
        "                         nb_val_samples = 2000)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "test_set.reset()\n",
        "pred=classifier.predict_generator(test_set,verbose=1)\n",
        "#pred = list(map(round,pred))\n",
        "pred[pred > .5] = 1\n",
        "pred[pred <= .5] = 0\n",
        "\n",
        "print('prediction gecti')\n",
        "#labels = (training_set.class_indices)\n",
        "\n",
        "test_labels = []\n",
        "\n",
        "for i in range(0,int(203)):\n",
        "    test_labels.extend(np.array(test_set[i][1]))\n",
        "    \n",
        "print('test_labels')\n",
        "print(test_labels)\n",
        "\n",
        "#labels = (training_set.class_indices)\n",
        "'''\n",
        "idx = []  \n",
        "for i in test_set:\n",
        "    ixx = (test_set.batch_index - 1) * test_set.batch_size\n",
        "    ixx = test_set.filenames[ixx : ixx + test_set.batch_size]\n",
        "    idx.append(ixx)\n",
        "    print(i)\n",
        "    print(idx)\n",
        "'''\n",
        "dosyaisimleri = test_set.filenames\n",
        "#abc = test_set.\n",
        "#print(idx)\n",
        "#test_labels = test_set.\n",
        "sonuc = pd.DataFrame()\n",
        "sonuc['dosyaisimleri']= dosyaisimleri\n",
        "sonuc['tahminler'] = pred\n",
        "sonuc['test'] = test_labels   \n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "cm = confusion_matrix(test_labels, pred)\n",
        "print (cm)"
      ],
      "metadata": {
        "id": "6nslRi8jObRc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}